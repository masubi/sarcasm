{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.models import Model, Sequential, model_from_yaml\n",
    "from keras.layers.core import Dense, Dropout, Activation, Masking\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Flatten, Bidirectional, Input\n",
    "from keras import regularizers\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.constraints import unitnorm, nonneg\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "import string\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, RemoteMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_to_index shape: 22583\n",
      "index_to_word shape: 22583\n",
      "word_to_vec_map shape: 22583\n"
     ]
    }
   ],
   "source": [
    "# labeling variables\n",
    "model_name = datetime.datetime.now().strftime(\"twitter_sentiment.%Y%m%dT%H%M.v\")\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "# data variables\n",
    "TRAINING_EXAMPLES=1575116\n",
    "SUBSET_SIZE = 1000000\n",
    "HOLDOUT_SIZE = 200\n",
    "max_features = 5003\n",
    "maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "\n",
    "# TODO:  fill in w/ data\n",
    "TRAIN_DATASET = [\"./data/sentiment/training.1600000.processed.noemoticon.csv\"] #e.g. [\"/home/masubi/jupyter/training.1600000.processed.noemoticon.csv\"]\n",
    "TEST_DATASET = [\"./data/sentiment/testdata.manual.2009.06.14.csv\"]  #e.g. [\"/home/masubi/jupyter/testdata.manual.csv\"]\n",
    "DICTIONARY = \"\"      #e.g. \"/home/masubi/jupyter/dictionary.json\"\n",
    "WORD_EMBEDDINGS = \"./data/sentiment/twitter_corpus.vec\" #e.g. \"/home/masubi/fastText/result/twitter_corpus.vec\"\n",
    "\n",
    "\n",
    "#If use w/ existing models\n",
    "load_weights=False\n",
    "weights_file=\"model_name\"+'-weights.hdf5'\n",
    "\n",
    "# sample for training index\n",
    "# I do this because the big twitter training set is sorted by label.\n",
    "# If you take the first 100k examples they will all be one class.\n",
    "# After you have your X & Y, the fit method will peform a shuffle for each epoch.\n",
    "def file_gen(file_path):\n",
    "    exclude = re.escape(re.sub(r\"[\\-\\_]\", \"\", string.punctuation))\n",
    "    \n",
    "    for i in file_path:\n",
    "        df = pd.read_csv(i, header=None,\n",
    "                         names=['label', 'id', 'date', 'query', \"user\", 'tweet'], encoding='latin-1')\n",
    "        df = df.ix[(df.label.notnull()) & (df.tweet.str.count(\" \") > 3), :]\n",
    "        df['tweet'] = df.tweet.astype(str).str.lower()             .str.replace(\"[_-]\", ' ')             .str.replace(\"\\'\", '')             .str.replace(\"\\.\", ' ')             .str.replace(\"at&amp;t\", \"at&t\")\n",
    "        yield df\n",
    "        \n",
    "#\n",
    "# word embeddings\n",
    "#\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(WORD_EMBEDDINGS)\n",
    "\n",
    "print(\"word_to_index shape: \"+str(len(word_to_index)))\n",
    "print(\"index_to_word shape: \"+str(len(index_to_word)))\n",
    "print(\"word_to_vec_map shape: \"+str(len(word_to_vec_map)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:33: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/indexing.py:822: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  retval = getattr(retval, self.name)._getitem_axis(key, axis=i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  orig x_train_all.shape: (1520164, 100)\n",
      "  orig y_train_all.shape: (1520164,)\n",
      "  orig x_test_all.shape: (476, 100)\n",
      "  orig y_test_all.shape: (476,)\n",
      "Loading data complete!\n",
      "Combine and Shuffle test w/ train Data\n",
      "  x_y_train.shape: (1520164, 101)\n",
      "  x_y_test.shape: (476, 101)\n",
      "  shuffled_train.shape:(1520364, 101)\n",
      "Generate subset size: (1000000, 100)\n",
      "  x_train.shape: (999800, 100)\n",
      "  y_train.shape: (999800, 1)\n",
      "  x_holdout.shape: (200, 100)\n",
      "  y_holdout.shape: (200, 1)\n",
      "positive examples in y_train: 496733\n",
      "positive examples in y_holdout: 107\n",
      "999800 train sequences\n",
      "Pad sequences (samples x time)\n",
      "One x_Training Example:\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0 22473 21855  7276 20702 22033 16414 18552 22228\n",
      " 12871  8082 14712 20358]\n",
      "One y_Training Example:\n",
      " [0]\n",
      "x_test.shape: (276, 100)\n",
      "y_test.shape: (276, 1)\n",
      "One x_Test Example:\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0 20485\n",
      " 21644  8847 16822 17517 20204 21137  8847 21651 22167   438 15319  7088\n",
      " 16062 13883 16027 22283]\n",
      "One y_Test Example:\n",
      " [0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "#Write corpus to file\n",
    "\n",
    "# file_path = file to load\n",
    "# load raw data\n",
    "def load_word_data(file_path):\n",
    "    df = pd.concat(file_gen(file_path))\n",
    "    records = [line for line in df['tweet'].values]    \n",
    "    return records\n",
    "\n",
    "# test\n",
    "# corpus_as_array = load_word_data(TEST_DATASET)\n",
    "\n",
    "def clean_line(line):\n",
    "    line = \" \".join(line.split())\n",
    "    line = line.strip()\n",
    "    line = line.replace(\"[_-]\", ' ')\n",
    "    line = line.replace(\"\\'\", '')\n",
    "    line = line.replace(\"at&amp;t\", \"at&t\")\n",
    "    return line\n",
    "\n",
    "#customized code\n",
    "def write_corpus_to_file(fileName):\n",
    "    F = open(fileName, \"w\")\n",
    "    print(\"reading datasets ...\")\n",
    "    corpus_test = load_word_data(TEST_DATASET)\n",
    "    corpus_train = load_word_data(TRAIN_DATASET)\n",
    "    for l in corpus_test:\n",
    "        F.write(clean_line(l))\n",
    "    print(\"corpus_test written to '\"+fileName+\"'\")\n",
    "    for l in corpus_train:\n",
    "        F.write(clean_line(l))\n",
    "    print(\"corpus_train written to '\"+fileName+\"'\")\n",
    "    F.close()\n",
    "        \n",
    "#write_corpus_to_file(\"./corpus.txt\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# file_path = file to load\n",
    "# return tweets tweets as indices\n",
    "def get_data(file_path):\n",
    "    df = pd.concat(file_gen(file_path))\n",
    "    \n",
    "    #  see list comprehension for how this works\n",
    "    #    equivalent to below:\n",
    "    #\n",
    "    #  for line in df['tweet'].values\n",
    "    #      for word in lines.split(\" \")\n",
    "    #          if(word in word_to_index.keys())\n",
    "    #             word_to_index[word]\n",
    "    records = [[word_to_index[word] for word in clean_line(line).split(\" \") if (word in word_to_index.keys())]\n",
    "               for line in df['tweet'].values]\n",
    "    \n",
    "    return sequence.pad_sequences(records, maxlen=maxlen), np.where(df.label.astype(int) >= 2, 1, 0)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Loading data...\")\n",
    "x_train_all, y_train_all = get_data(TRAIN_DATASET)\n",
    "x_test_all, y_test_all = get_data(TEST_DATASET)\n",
    "print(\"  orig x_train_all.shape: \"+str(x_train_all.shape))\n",
    "print(\"  orig y_train_all.shape: \"+str(y_train_all.shape))\n",
    "print(\"  orig x_test_all.shape: \"+str(x_test_all.shape))\n",
    "print(\"  orig y_test_all.shape: \"+str(y_test_all.shape))\n",
    "print(\"Loading data complete!\")\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Combine & Shuffling Train and Test Data\n",
    "# ------------------------------------------\n",
    "# Manual investigation shows that the test dataset contains negative, neutral, positive sentiments\n",
    "# so need to combine and shuffle this data\n",
    "\n",
    "# combine x and y cols\n",
    "def concat_cols(x,y):\n",
    "    y = y.reshape(y.shape[0],1)\n",
    "    return np.concatenate((x,y), axis=1)\n",
    "\n",
    "# split x and y\n",
    "def split_cols(a):\n",
    "    x_res = a[:,0:a.shape[1]-1]\n",
    "    y_res = a[:,a.shape[1]-1:a.shape[1]]\n",
    "    return x_res, y_res\n",
    "\n",
    "# shuffle rows of a\n",
    "def shuffle(a, subset_size):\n",
    "    subset = a[np.random.choice(a.shape[0], subset_size, replace=False), :]\n",
    "    return subset\n",
    "\n",
    "# generates random subset of x,y\n",
    "# x: x records\n",
    "# y: y labels\n",
    "# subset_size:  subset size\n",
    "def generate_subset(x, y, subset_size):    \n",
    "    combined = concat_cols(x,y)\n",
    "    assert( subset_size < combined.shape[0])\n",
    "    subset = shuffle(combined, subset_size)\n",
    "    x_res = subset[:,0:x.shape[1]]\n",
    "    y_res = subset[:,subset.shape[1]-1:subset.shape[1]]\n",
    "    return x_res, y_res\n",
    "\n",
    "print(\"Combine and Shuffle test w/ train Data\")\n",
    "test_subset_size=200 #size of test data to shuffle into \n",
    "\n",
    "#combine x_train_all and y_train_all\n",
    "x_y_train = concat_cols(x_train_all, y_train_all)\n",
    "print(\"  x_y_train.shape: \"+str(x_y_train.shape))\n",
    "\n",
    "#combine x_test_all, y_test_all\n",
    "x_y_test = concat_cols(x_test_all, y_test_all)\n",
    "print(\"  x_y_test.shape: \"+str(x_y_test.shape))\n",
    "\n",
    "# get subset of test\n",
    "test_subset_to_shuffle_w_train = x_y_test[0:test_subset_size, :]\n",
    "# get remaining x,y test\n",
    "x_test, y_test = split_cols(x_y_test[test_subset_size:x_y_test.shape[0], :])\n",
    "\n",
    "# combine and shuffle w/ train\n",
    "shuffled_train = np.concatenate((x_y_train,test_subset_to_shuffle_w_train), axis=0)\n",
    "shuffled_train = shuffle(shuffled_train, shuffled_train.shape[0])\n",
    "print(\"  shuffled_train.shape:\"+str(shuffled_train.shape))\n",
    "x_train_all, y_train_all = split_cols(shuffled_train)\n",
    "\n",
    "\n",
    "x_train_subset, y_train_subset = generate_subset(x_train_all, y_train_all, SUBSET_SIZE)\n",
    "train_end_index = x_train_subset.shape[0]-HOLDOUT_SIZE\n",
    "holdout_end_index = x_train_subset.shape[0]\n",
    "\n",
    "print(\"Generate subset size: \"+str(x_train_subset.shape))\n",
    "x_train = x_train_subset[0:train_end_index,:]\n",
    "y_train = y_train_subset[0:train_end_index,:]\n",
    "print(\"  x_train.shape: \"+str(x_train.shape))\n",
    "print(\"  y_train.shape: \"+str(y_train.shape))\n",
    "#print(\"    x_train: \"+str(x_train))\n",
    "#print(\"    y_train: \"+str(y_train))\n",
    "x_holdout = x_train_subset[train_end_index:holdout_end_index,:]\n",
    "y_holdout = y_train_subset[train_end_index:holdout_end_index,:]\n",
    "print(\"  x_holdout.shape: \"+str(x_holdout.shape))\n",
    "print(\"  y_holdout.shape: \"+str(y_holdout.shape))\n",
    "#print(\"    x_holdout: \"+str(x_holdout))\n",
    "#print(\"    y_holdout: \"+str(y_holdout))\n",
    "print(\"positive examples in y_train:\", y_train.sum())\n",
    "print(\"positive examples in y_holdout:\", y_holdout.sum())\n",
    "print(len(x_train), 'train sequences')\n",
    "print(\"Pad sequences (samples x time)\")\n",
    "print(\"One x_Training Example:\\n\", x_train[0, :])\n",
    "print(\"One y_Training Example:\\n\", y_train[0, :])\n",
    "\n",
    "print(\"x_test.shape: \"+str(x_test.shape))\n",
    "print(\"y_test.shape: \"+str(y_test.shape))\n",
    "print(\"One x_Test Example:\\n\", x_test[0, :])\n",
    "print(\"One y_Test Example:\\n\", y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model ... twitter_sentiment.20191114T0846.v\n",
      "shape mismatch\n",
      "emb_matrix: (100,)\n",
      "word_to_vec_map[word]: (1,)\n",
      "word: 22582\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:182: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2258400   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 256)          234496    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,920,161\n",
      "Trainable params: 2,920,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Build model complete\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:353: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 999800 samples, validate on 200 samples\n",
      "Epoch 1/20\n",
      " 37408/999800 [>.............................] - ETA: 2:13:22 - loss: 8.8042 - accuracy: 0.5593 - f1: 0.3870 - precision: 0.5542 - recall: 0.3448"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e8e7e8ac94d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_holdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_holdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;31m#                    callbacks=[checkpointer, tensorboard])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                      callbacks=[checkpointer])\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training complete'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3508\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3509\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3510\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3512\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    571\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# In[7]:\n",
    "\n",
    "\n",
    "#for i in range(len(x_test)):\n",
    "#    sentence = indicesToSentence(x_test[i])\n",
    "#    print(\"i: \"+str(i)+ \", \"+sentence)\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "#\n",
    "# Experiment Metrics\n",
    "#\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2*((p*r)/(p+r+K.epsilon()))\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "print('Build model ... ' + model_name)\n",
    "\n",
    "batch_size=32\n",
    "epochs=20\n",
    "optimizer=SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        if(emb_matrix[index, :].shape == word_to_vec_map[word].shape):\n",
    "            emb_matrix[index, :] = word_to_vec_map[word]\n",
    "        else:\n",
    "            print(\"shape mismatch\")\n",
    "            print(\"emb_matrix: \"+str(emb_matrix[index, :].shape))\n",
    "            print(\"word_to_vec_map[word]: \"+str(word_to_vec_map[word].shape))\n",
    "            print(\"word: \"+word)\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "\n",
    "\n",
    "maxLen = maxlen#len(max(x_train, key=len).split())\n",
    "\n",
    "# Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "#sentence_indices = np.zeros(input_shape, dtype='int32')\n",
    "sentence_indices = Input(shape=(maxLen,))\n",
    "\n",
    "\n",
    "# Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "\n",
    "# Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "embeddings = embedding_layer(inputs=sentence_indices)   \n",
    "\n",
    "X = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=regularizers.l2(0.01)))(embeddings)\n",
    "X = Bidirectional(LSTM(128, kernel_regularizer=regularizers.l2(0.01)))(X)\n",
    "X = Dense(128, activation='relu')(X)\n",
    "X = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "# Create Model instance which converts sentence_indices into X.\n",
    "model = Model(inputs=[sentence_indices], outputs=[X])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy', f1, precision, recall])\n",
    "print(model.summary())\n",
    "\n",
    "if(load_weights):\n",
    "    print(\"loading weights from: \"+weights_file)\n",
    "    model.load_weights(weights_file)\n",
    "    print(\"weights: \")\n",
    "    print(str(model.get_weights()))\n",
    "    print(\"loading weights complete!\")\n",
    "\n",
    "print('Build model complete')\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "print('Training...')\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='./'+model_name+'-weights.hdf5', verbose=2, save_best_only=True)\n",
    "\n",
    "#tensorboard = TensorBoard(log_dir='./logs/'+model_name, \n",
    "#                          histogram_freq=0,\n",
    "#                          write_graph=True, \n",
    "#                          write_images=True)\n",
    "\n",
    "\n",
    "history = model.fit(x_train, \n",
    "                    y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    verbose=1, \n",
    "                    validation_data=(x_holdout, y_holdout), \n",
    "#                    callbacks=[checkpointer, tensorboard])\n",
    "                     callbacks=[checkpointer])\n",
    "    \n",
    "print('Training complete')\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "print('Testing...')\n",
    "\n",
    "print('  model.metrics_names: '+str(model.metrics_names))\n",
    "print('x_test.shape:', x_test.shape)\n",
    "loss, accuracy, f1, precision, recall = model.evaluate(x_test, y_test, batch_size=32)\n",
    "metrics = {\n",
    "    'loss':loss,\n",
    "    'accuracy':accuracy,\n",
    "    'f1': f1,\n",
    "    'precision': precision,\n",
    "    'recall': recall\n",
    "}\n",
    "\n",
    "\n",
    "print('loss: '+str(loss))\n",
    "print('accuracy: '+str(accuracy))\n",
    "print('f1: '+str(f1))\n",
    "print('precision: '+str(precision))\n",
    "print('recall: ' + str(recall))\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "#\n",
    "# debug predictions\n",
    "#\n",
    "def indicesToSentence(indices):\n",
    "    result = \"\"\n",
    "    for i in range(len(indices)):\n",
    "        if(indices[i] != \"0\" and indices[i] in index_to_word.keys()): \n",
    "            result=result+\" \"+str(index_to_word[indices[i]])\n",
    "    return result\n",
    "\n",
    "error_file = model_name+\"_test_errors.txt\"\n",
    "def print_errors(x_test, y_test, model):   \n",
    "    F = open(error_file, \"w\")\n",
    "    pred = model.predict(x_test)\n",
    "    errorCount = 0\n",
    "    for i in range(len(x_test)):\n",
    "        sentence = indicesToSentence(x_test[i])\n",
    "        prediction = pred[i]\n",
    "        actual = y_test[i]\n",
    "        if(abs(prediction - actual) > .5):\n",
    "            resultStr = \"i: \"+str(i)+\" pred: \"+str(prediction)+\" actual: \"+str(actual) + \" sentence: \"+sentence\n",
    "            print(resultStr)\n",
    "            F.write(resultStr+\"\\n\")\n",
    "            errorCount=errorCount+1\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"errorCount: \"+str(errorCount))\n",
    "    print(\"total: \"+str(len(x_test)))\n",
    "    print(\"sample test sentence: \"+str(indicesToSentence(x_test[0])))   \n",
    "    F.close()\n",
    "\n",
    "success_file = model_name+\"_test_success.txt\"\n",
    "def print_successes(x_test, y_test, model):   \n",
    "    F = open(success_file, \"w\")\n",
    "    pred = model.predict(x_test)\n",
    "    errorCount = 0\n",
    "    for i in range(len(x_test)):\n",
    "        sentence = indicesToSentence(x_test[i])\n",
    "        prediction = pred[i]\n",
    "        actual = y_test[i]\n",
    "        if(abs(prediction - actual) < .5):\n",
    "            resultStr = \"i: \"+str(i)+\" pred: \"+str(prediction)+\" actual: \"+str(actual) + \" sentence: \"+sentence\n",
    "            print(resultStr)\n",
    "            F.write(resultStr+\"\\n\")\n",
    "            errorCount=errorCount+1\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"sucessCount: \"+str(errorCount))\n",
    "    print(\"total: \"+str(len(x_test)))\n",
    "    print(\"sample test sentence: \"+str(indicesToSentence(x_test[0])))  \n",
    "    F.close()\n",
    "    \n",
    "print_errors(x_test, y_test, model)    \n",
    "print_successes(x_test, y_test, model)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "print('Save Model')\n",
    "model.save_weights(model_name+\".hdf5\", overwrite=True)\n",
    "yaml_string = model.to_yaml()\n",
    "open(model_name+'.yaml', 'w+').write(yaml_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
